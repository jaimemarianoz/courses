RED HAT CEPH STORAGE ARCHITECTURE AND ADMINISTRATION CEPH125
------------------------------------------------------------

CHAPTER 2
DEPLOYING RED HAT CEPH STORAGE
-------------------------------


Pagina 38
GUIDED EXERCISE - DEPLOYING CEPH USING ANSIBLE
----------------------------------------------

[student@workstation ~]$ ssh servera

[student@servera ~]$ ssh-keygen -t rsa -f ~/.ssh/id_rsa -N ""

[student@servera ~]$ for i in a b c d e 
do 
ssh-copy-id student@server$i 
ssh-copy-id ceph@server$i 
done 

Ingresar para student el password student ; para ceph el password redhat

[student@servera ~]$ sudo vi /usr/share/ceph-ansible/ansible.cfg
[defaults]
...output omitted...
roles_path = ./roles
# Be sure the user running Ansible has permissions on the logfile
log_path = /tmp/ansible.log   # Modificar la ubicacion de los logs
deprecation_warnings = false  # Descartar mensajes de alertas conocidas


[student@servera ~]$ sudo vi /etc/ansible/hosts
[mons]
server[c:e]
[mgrs]
server[c:e]

[student@servera ~]$ ansible mons -m ping

[student@servera ~]$ ansible mgrs -m ping

[student@servera ~]$ ansible mons -m command -a id

[student@servera ~]$ ansible mons -m command -a id -b


[student@servera ~]$ cd /usr/share/ceph-ansible/
[student@servera ceph-ansible]$ sudo cp site.yml.sample site.yml
[student@servera ceph-ansible]$ sudo vi site.yml
...
- hosts: osds
gather_facts: false
serial: 1 #Agregar esta linea

[student@servera ceph-ansible]$ cat group_vars/all.yml
---
fetch_directory: ~/ceph-ansible-keys
ntp_service_enabled: false
ceph_origin: repository
ceph_repository: rhcs
ceph_rhcs_version: "3"
ceph_repository_type: cdn
rbd_cache: true
rbd_cache_writethrough_until_flush: false
rbd_client_directories: false
monitor_interface: eth0
journal_size: 1024
public_network: 172.25.250.0/24
cluster_network: "{{ public_network }}"
ceph_conf_overrides:
  global:
    mon_osd_allow_primary_affinity: 1
    mon_clock_drift_allowed: 0.5
    osd_pool_default_size: 2
    osd_pool_default_min_size: 1
    mon_pg_warn_min_per_osd: 0
    mon_pg_warn_max_per_osd: 0
    mon_pg_warn_max_object_skew: 0
  client:
    rbd_default_features: 1

[student@servera group_vars]$ cd ..
[student@servera ceph-ansible]$ ansible-playbook site.yml

[student@servera ceph-ansible]$ ssh ceph@serverc

Validacion de la Instalacion
----------------------------
[ceph@serverc ~]$ ceph -s

[ceph@serverc ~]$ cat /etc/ceph/ceph.conf

[ceph@serverc ~]$ ps aux | grep ceph-mon

[ceph@serverc ~]$ exit

[student@servera ceph-ansible]$ sudo vi group_vars/osds.yml
devices:
- /dev/vdb
osd_scenario: "collocated"

[student@servera ceph-ansible]$ sudo vi /etc/ansible/hosts
[mons]
server[c:e]
[mgrs]
server[c:e]
[osds]
server[c:e]

[student@servera ceph-ansible]$ ansible-playbook site.yml

[student@servera ceph-ansible]$ ssh ceph@serverc
[ceph@serverc ~]$ ceph -s

[ceph@serverc ~]$ ceph -w

[ceph@serverc ~]$ ps aux | grep ceph-mon

[ceph@serverc ~]$ sudo systemctl stop ceph-mon.target
[ceph@serverc ~]$ ps aux | grep ceph-mon

[ceph@serverc ~]$ ceph -s

[ceph@serverc ~]$ sudo systemctl start ceph-mon.target
[ceph@serverc ~]$ ps aux | grep ceph-mon

[ceph@serverc ~]$ ceph -s

[ceph@serverc ~]$ sudo systemctl restart ceph-mon.target
[ceph@serverc ~]$ ps aux | grep ceph-mon

[ceph@serverc ~]$ ceph -s

[ceph@serverc ~]$ ps aux | grep ceph-osd

[ceph@serverc ~]$ sudo systemctl stop ceph-osd.target
[ceph@serverc ~]$ ps aux | grep ceph-osd
[ceph@serverc ~]$ ceph -s

[ceph@serverc ~]$ ceph osd tree

[ceph@serverc ~]$ sudo systemctl start ceph-osd.target
[ceph@serverc ~]$ ps aux | grep ceph-osd
[ceph@serverc ~]$ ceph osd tree

[ceph@serverc ~]$ sudo systemctl stop ceph-osd@0
[ceph@serverc ~]$ ceph osd tree

[ceph@serverc ~]$ sudo systemctl start ceph-osd@0
[ceph@serverc ~]$ ceph osd tree

[ceph@serverc ~]$ ceph -s

[ceph@serverc ~]$ ceph -v

[ceph@serverc ~]$ exit

[student@servera ceph-ansible]$ sudo vi group_vars/clients.yml
copy_admin_key: true


[student@servera ceph-ansible]$ sudo vi /etc/ansible/hosts
[mons]
server[c:e]
[mgrs]
server[c:e]
[osds]
server[c:e]
[clients]
servera

[student@servera ceph-ansible]$ ansible-playbook --limit=clients site.yml

[student@servera ceph-ansible]$ ssh ceph@servera
[ceph@servera ~]$ ceph -s

[ceph@servera ~]$ logout
[student@servera ceph-ansible]$ logout

PAGINA 54
GUIDED EXERCISE EXPANDING A RED HAT CEPH STORAGE CLUSTER'S CAPACITY
-------------------------------------------------------------------

[student@workstation ~]$ ssh student@servera
[student@servera ~]$ cd /usr/share/ceph-ansible
[student@servera ceph-ansible]$ sudo vi group_vars/osds.yml
devices:
- /dev/vdb
- /dev/vdc
- /dev/vdd

[student@servera ceph-ansible]$ ansible-playbook site.yml

[student@servera ceph-ansible]$ ssh ceph@serverc
[ceph@serverc ~]$ ceph osd stat
[ceph@serverc ~]$ ceph -s

[ceph@serverc ~]$ ceph osd tree
[ceph@serverc ~]$ ceph osd df

[ceph@serverc ~]$ logout

PAGINA57
LAB DEPLOYING RED HAT CEPH STORAGE
----------------------------------

[student@workstation ~]$ ssh servera

[student@servera ~]$ ssh-copy-id student@serverf

[student@servera ~]$ ssh-copy-id ceph@serverf

[student@servera ~]$ cp -r /usr/share/ceph-ansible ~/serverf-cluster

[student@servera ~]$ vim /home/student/serverf-cluster/ansible.cfg

inventory = ./serverf-cluster-hosts

[student@servera ~]$ cd ~/serverf-cluster
[student@servera serverf-cluster]$ vi serverf-cluster-hosts
[mons]
serverf
[mgrs]
serverf
[osds]
serverf

[student@servera serverf-cluster]$ ansible mons -i serverf-cluster-hosts -m ping

[student@servera serverf-cluster]$ ansible mgrs -i serverf-cluster-hosts -m ping

[student@servera serverf-cluster]$ ansible osds -i serverf-cluster-hosts -m ping

[student@servera serverf-cluster]$ ansible mons -i serverf-cluster-hosts -m command -a id

[student@servera serverf-cluster]$ ansible mons -i serverf-cluster-hosts -m command -a id -b

In /home/student/serverf-cluster, copy group_vars/all.yml.sample to
group_vars/all.yml and edit it to meet the following specification:

[student@servera serverf-cluster]$ vi group_vars/all.yml \
fetch_directory: ~/ceph-ansible-serverf-keys
ntp_service_enabled: false
ceph_origin: repository
ceph_repository: rhcs
ceph_rhcs_version: "3"
ceph_repository_type: cdn
rbd_cache: "true"
rbd_cache_writethrough_until_flush: "false"
rbd_client_directories: false
monitor_interface: eth0
journal_size: 1024
public_network: 172.25.250.0/24
cluster_network: "{{ public_network }}"
common_single_host_mode: true
ceph_conf_overrides:
  global:
    mon_osd_allow_primary_affinity: 1
    mon_clock_drift_allowed: 0.5
    osd_pool_default_size: 2
    osd_pool_default_min_size: 1
    mon_pg_warn_min_per_osd: 0
    mon_pg_warn_max_per_osd: 0
    mon_pg_warn_max_object_skew: 0
  client:
    rbd_default_features: 1

[student@servera serverf-cluster]$ cd group_vars
[student@servera group_vars]$ cp -f osds.yml.sample osds.yml
[student@servera group_vars]$ vi osds.yml
devices:
- /dev/vdb
- /dev/vdc
- /dev/vdd
osd_scenario: "collocated"

[student@servera group_vars]$ cd ..
[student@servera serverf-cluster]$ cp -f site.yml.sample site.yml
[student@servera serverf-cluster]$ ansible-playbook -i serverf-cluster-hosts \
> site.yml

[student@servera serverf-cluster]$ ssh ceph@serverf
[ceph@serverf ~]$ ceph -s

[student@workstation ~]$ lab deploy-review grade


CHAPTER 3 - CONFIGURING RED HAT CEPH STORAGE
--------------------------------------------

Pagina 76
GUIDED EXERCISE - MANAGING REPLICATED POOLS
-------------------------------------------
[student@workstation ~]$ ssh ceph@serverc
[ceph@serverc ~]$ ceph health
[ceph@serverc ~]$ ceph osd pool create whirlpool 64
[ceph@serverc ~]$ ceph osd pool application enable whirlpool rbd
[ceph@serverc ~]$ ceph osd pool ls
[ceph@serverc ~]$ ceph df

[ceph@serverc ~]$ ceph osd pool rename whirlpool mypool
[ceph@serverc ~]$ ceph osd pool ls detail

[ceph@serverc ~]$ ceph osd pool set mypool size 3

[ceph@serverc ~]$ rados -p mypool -N system put testconf /etc/ceph/ceph.conf
[ceph@serverc ~]$ rados -p mypool put testkey /etc/ceph/ceph.client.admin.keyring

[ceph@serverc ~]$ rados -p mypool -N system ls
testconf
[ceph@serverc ~]$ rados -p mypool ls
testkey
[ceph@serverc ~]$ rados -p mypool --all ls
system testconf
       testkey

[ceph@serverc ~]$ ceph osd pool delete mypool

[ceph@serverc ~]$ ceph osd pool delete mypool mypool --yes-i-really-really-mean-it

Pagina 84
GUIDED EXERCISE - MANAGING ERASURE CODED POOLS
----------------------------------------------
[ceph@serverc ~]$ ceph osd erasure-code-profile ls

[ceph@serverc ~]$ ceph osd erasure-code-profile get default

[ceph@serverc ~]$ ceph osd erasure-code-profile set ceph125 k=3 m=2 \
crush-failure-domain=osd

[ceph@serverc ~]$ ceph osd erasure-code-profile get ceph125

[ceph@serverc ~]$ ceph osd pool create myecpool 64 64 erasure ceph125

[ceph@serverc ~]$ ceph osd pool application enable myecpool rgw

[ceph@serverc ~]$ ceph df

[ceph@serverc ~]$ ceph osd pool ls detail

[ceph@serverc ~]$ ceph health

[ceph@serverc ~]$ rados -p myecpool put mytest /usr/share/dict/words

[ceph@serverc ~]$ ceph df

[ceph@serverc ~]$ rados -p myecpool get mytest /tmp/words

[ceph@serverc ~]$ diff /tmp/words /usr/share/dict/words

Pagina 93
GUIDED EXERCISE - MODIFYING SETTINGS IN THE CONFIGURATION FILE
--------------------------------------------------------------

[ceph@serverc ~]$ ceph osd pool delete myecpool

[ceph@serverc ~]$ ceph osd pool delete myecpool myecpool \
--yes-i-really-really-mean-it

ceph@serverc ~]$ ceph daemon mon.serverc config show

[ceph@serverc ~]$ ceph daemon mon.serverc config get mon_allow_pool_delete
{
    "mon_allow_pool_delete": "false"
}

[ceph@serverc ~]$ ceph daemon osd.0 config get debug_osd

[ceph@serverc ~]$ cp /etc/ceph/ceph.conf /etc/ceph/ceph.conf.save

[student@servera ~]$ sudo vi /usr/share/ceph-ansible/group_vars/all.yml

...
ceph_conf_overrides:
  global:
    mon_osd_allow_primary_affinity: 1
    mon_clock_drift_allowed: 0.5
    osd_pool_default_size: 2
    osd_pool_default_min_size: 1
    mon_pg_warn_min_per_osd: 0
    mon_pg_warn_max_per_osd: 0
    mon_pg_warn_max_object_skew: 0
    mon_allow_pool_delete: true # add this line
  osd:                          # add this line  
    debug_osd: 10               # add this line
  client:
    rbd_default_features: 1
    debug_ms: 1                 # add this line
...

[student@servera ~]$ cd /usr/share/ceph-ansible
[student@servera ceph-ansible]$ ansible-playbook site.yml

[ceph@serverc ~]$ diff /etc/ceph/ceph.conf /etc/ceph/ceph.conf.save

[ceph@serverc ~]$ sudo systemctl restart ceph-mon.target
[ceph@serverc ~]$ ceph daemon mon.serverc config get mon_allow_pool_delete

[ceph@serverc ~]$ ceph osd pool delete myecpool myecpool \
--yes-i-really-really-mean-it

[student@servera ~]$ sudo vi /usr/share/ceph-ansible/group_vars/all.yml

...
ceph_conf_overrides:
  global:
    mon_osd_allow_primary_affinity: 1
    mon_clock_drift_allowed: 0.5
    osd_pool_default_size: 2
    osd_pool_default_min_size: 1
    mon_pg_warn_min_per_osd: 0
    mon_pg_warn_max_per_osd: 0
    mon_pg_warn_max_object_skew: 0
    mon_allow_pool_delete: true # add this line
  client:
    rbd_default_features: 1
...

[student@servera ~]$ cd /usr/share/ceph-ansible
[student@servera ceph-ansible]$ ansible-playbook site.yml


[ceph@serverc ~]$ systemctl restart ceph-osd@0.service

[ceph@serverc ~]$ ceph daemon osd.0 config get debug_osd
{
"debug_osd": "1/5"
}


Pagina 105
GUIDED EXERCISE - MANAGING CEPH AUTHENTICATION
----------------------------------------------

[ceph@serverc ~]$ ceph osd pool ls

[ceph@serverc ~]$ ceph auth get-or-create client.docedit \
mon 'allow r' \
osd 'allow rw pool=mypool namespace=docs' \
-o /etc/ceph/ceph.client.docedit.keyring

[ceph@serverc ~]$ ceph auth get-or-create client.docget \
mon 'allow r' \
osd 'allow r pool=mypool namespace=docs' \
-o /etc/ceph/ceph.client.docget.keyring

[ceph@serverc ~]$ ceph auth list

[ceph@serverc ~]$ scp /etc/ceph/ceph.client.docedit.keyring servera:/etc/ceph/

[ceph@serverc ~]$ scp /etc/ceph/ceph.client.docget.keyring servera:/etc/ceph/

[ceph@servera ~]$ rados --id docedit -p mypool -N docs put testdoc /etc/services
[ceph@servera ~]$ rados --id docget -p mypool -N docs get testdoc /tmp/test
[ceph@servera ~]$ diff /etc/services /tmp/test

[ceph@servera ~]$ rados --id docget -p mypool -N docs put mywritetest /etc/services
error putting mypool/mywritetest: (1) Operation not permitted


[ceph@serverc ~]$ ceph auth caps client.docget \
mon 'allow r' \
osd 'allow rw pool=mypool namespace=docs, allow rw pool=docarchive'

[ceph@serverc ~]$ rados --id docget -p mypool -N docs put mywritetest /etc/services

[ceph@serverc ~]$ rm /etc/ceph/ceph.client.docedit.keyring
[ceph@serverc ~]$ ssh servera rm /etc/ceph/ceph.client.docedit.keyring

[ceph@serverc ~]$ ceph auth del client.docedit

[ceph@serverc ~]$ rm /etc/ceph/ceph.client.docget.keyring
[ceph@serverc ~]$ ssh servera rm /etc/ceph/ceph.client.docget.keyring

[ceph@serverc ~]$ ceph auth del client.docget

Pagina 108
LAB - CONFIGURING RED HAT CEPH STORAGE
--------------------------------------
[student@workstation ~]$ lab ceph-config setup

[student@workstation ~]$ ssh ceph@serverc

[ceph@serverc ~]$ ceph osd pool delete mypool mypool \
--yes-i-really-really-mean-it

[ceph@serverc ~]$ ceph osd pool create blue 64

[ceph@serverc ~]$ ceph osd pool set blue size 3

[ceph@serverc ~]$ ceph osd pool application enable blue rbd

[ceph@serverc ~]$ ceph osd pool ls detail

[ceph@serverc ~]$ ceph osd erasure-code-profile ls

[ceph@serverc ~]$ ceph osd erasure-code-profile get default

[ceph@serverc ~]$ ceph osd pool create yellow 64 64 erasure default

[ceph@serverc ~]$ ceph osd pool application enable yellow rgw

[ceph@serverc ~]$ ceph osd pool ls detail

[ceph@serverc ~]$ ceph daemon mon.serverc config get mon_max_pg_per_osd
{
"mon_max_pg_per_osd": "200"
}

[student@servera ~]$ sudo vi /usr/share/ceph-ansible/group_vars/all.yml

...
ceph_conf_overrides:
  global:
    mon_osd_allow_primary_affinity: 1
    mon_clock_drift_allowed: 0.5
    osd_pool_default_size: 2
    osd_pool_default_min_size: 1
    mon_pg_warn_min_per_osd: 0
    mon_pg_warn_max_per_osd: 0
    mon_pg_warn_max_object_skew: 0
    mon_allow_pool_delete: false  # add this line
    mon_max_pg_per_osd: 400       # add this line
  client:
    rbd_default_features: 1
...

[student@servera ~]$ cd /usr/share/ceph-ansible
[student@servera ceph-ansible]$ ansible-playbook site.yml

[ceph@serverc ~]$ sudo systemctl restart ceph-mon.target
[ceph@serverc ~]$ ceph daemon mon.serverc config get mon_allow_pool_delete
{
"mon_allow_pool_delete": "false"
}

[ceph@serverc ~]$ ceph daemon mon.serverc config get mon_max_pg_per_osd
{
"mon_max_pg_per_osd": "400"
}

[ceph@serverc ~]$ ceph auth get-or-create client.wcolor \
mon 'allow r' \
osd 'allow rw pool=blue' \
-o /etc/ceph/ceph.client.wcolor.keyring

[ceph@serverc ~]$ ceph auth get-or-create client.rcolor \
mon 'allow r' \
osd 'allow r pool=blue object_prefix rgb_' \
-o /etc/ceph/ceph.client.rcolor.keyring

[ceph@serverc ~]$ scp /etc/ceph/ceph.client.wcolor.keyring servera:/etc/ceph/

[ceph@serverc ~]$ scp /etc/ceph/ceph.client.rcolor.keyring servera:/etc/ceph/

[ceph@servera ~]$ rados --id wcolor -p blue put rgb_colors /etc/DIR_COLORS.256color

[ceph@servera ~]$ rados --id rcolor -p blue get rgb_colors /tmp/color.out
[ceph@servera ~]$ diff /etc/DIR_COLORS.256color /tmp/color.out

[student@workstation ~]$ lab ceph-config grade

[student@workstation ~]$ lab ceph-config cleanup


CHAPTER 4 - PROVIDING BLOCK STORAGE WITH RBD
--------------------------------------------

Pagina 131
GUIDED EXERCISE - MANAGING RADOS BLOCK DEVICES
----------------------------------------------
[ceph@serverc ~]$ ceph health

[ceph@serverc ~]$ ceph osd pool create rbd 32 32

[ceph@serverc ~]$ rbd pool init rbd

[ceph@serverc ~]$ ceph df

[ceph@serverc ~]$ ceph auth get-or-create client.rbd.servera \
mon 'profile rbd' osd 'profile rbd' \
-o /etc/ceph/ceph.client.rbd.servera.keyring

[ceph@serverc ~]$ ceph auth list

[ceph@servera ~]$ scp serverc:/etc/ceph/ceph.client.rbd.servera.keyring \
/etc/ceph/ceph.client.rbd.servera.keyring

[ceph@servera ~]$ export CEPH_ARGS='--id=rbd.servera'

[ceph@servera ~]$ rbd ls

[ceph@servera ~]$ rbd create rbd/test --size=128M
[ceph@servera ~]$ rbd ls

[ceph@servera ~]$ rbd info rbd/test

[ceph@servera ~]$ sudo rbd --id rbd.servera map rbd/test

[ceph@servera ~]$ rbd showmapped

[ceph@servera ~]$ sudo mkfs.ext4 /dev/rbd0

[ceph@servera ~]$ sudo mkdir /mnt/rbd

[ceph@servera ~]$ sudo mount /dev/rbd0 /mnt/rbd

[ceph@servera ~]$ sudo chown ceph:ceph /mnt/rbd

[ceph@servera ~]$ df

[ceph@servera ~]$ dd if=/dev/zero of=/tmp/testrbd bs=10M count=1

[ceph@servera ~]$ cp /tmp/testrbd /mnt/rbd/test1

[ceph@servera ~]$ ls /mnt/rbd

[ceph@servera ~]$ df

[ceph@servera ~]$ ceph df

[ceph@servera ~]$ cp /tmp/testrbd /mnt/rbd/test2
[ceph@servera ~]$ cp /tmp/testrbd /mnt/rbd/test3
[ceph@servera ~]$ ls /mnt/rbd

[ceph@servera ~]$ df

[ceph@servera ~]$ ceph df

[ceph@servera ~]$ rados -p rbd put test /tmp/testrbd

[ceph@servera ~]$ ls /mnt/rbd

[ceph@servera ~]$ df

[ceph@servera ~]$ ceph df

[ceph@servera ~]$ rbd du rbd/test

[ceph@servera ~]$ rados -p rbd ls

[ceph@servera ~]$ sudo umount /mnt/rbd

[ceph@servera ~]$ sudo rbd --id rbd.servera unmap /dev/rbd0

[ceph@servera ~]$ rbd showmapped

[ceph@servera ~]$ rbd rm rbd/test

[ceph@servera ~]$ rados -p rbd rm test

[ceph@servera ~]$ rados -p rbd ls

[ceph@servera ~]$ rbd create rbd/snaptest --size=128M

[ceph@servera ~]$ sudo rbd --id rbd.servera map rbd/snaptest

[ceph@servera ~]$ rbd showmapped

[ceph@servera ~]$ sudo mkfs.ext4 /dev/rbd0

[ceph@servera ~]$ rbd snap create rbd/snaptest@firstsnap

[ceph@servera ~]$ sudo rbd --id rbd.servera map rbd/snaptest@firstsnap

[ceph@servera ~]$ rbd showmapped

[ceph@servera ~]$ sudo blockdev --getro /dev/rbd0

[ceph@servera ~]$ sudo blockdev --getro /dev/rbd1

[ceph@servera ~]$ sudo mkdir /mnt/org-rbd
[ceph@servera ~]$ sudo mount /dev/rbd0 /mnt/org-rbd
[ceph@servera ~]$ sudo chown ceph:ceph /mnt/org-rbd
[ceph@servera ~]$ df

[ceph@servera ~]$ ls /mnt/org-rbd

[ceph@servera ~]$ dd if=/dev/zero of=/mnt/org-rbd/file0 bs=1M count=10

[ceph@servera ~]$ ls /mnt/org-rbd

[ceph@servera ~]$ df

[ceph@servera ~]$ sudo mkdir /mnt/snap-rbd
[ceph@servera ~]$ sudo mount /dev/rbd1 /mnt/snap-rbd

[ceph@servera ~]$ sudo chown ceph:ceph /mnt/snap-rbd

[ceph@servera ~]$ df

[ceph@servera ~]$ ls /mnt/org-rbd

[ceph@servera ~]$ ls /mnt/snap-rbd

[ceph@servera ~]$ sudo umount /mnt/snap-rbd
[ceph@servera ~]$ sudo umount /mnt/org-rbd
[ceph@servera ~]$ df

[ceph@servera ~]$ sudo rbd --id rbd.servera unmap /dev/rbd0

[ceph@servera ~]$ sudo rbd --id rbd.servera unmap /dev/rbd1
[ceph@servera ~]$ rbd showmapped
[ceph@servera ~]$ rbd snap purge rbd/snaptest

[ceph@servera ~]$ rbd rm rbd/snaptest

[ceph@servera ~]$ rbd create rbd/clonetest --size=128M
[ceph@servera ~]$ rbd info rbd/clonetest

[ceph@servera ~]$ sudo rbd --id rbd.servera map rbd/clonetest

[ceph@servera ~]$ rbd showmapped

[ceph@servera ~]$ sudo mkfs.ext4 /dev/rbd0

[ceph@servera ~]$ sudo mkdir /mnt/source
[ceph@servera ~]$ sudo mount /dev/rbd0 /mnt/source
[ceph@servera ~]$ sudo chown ceph:ceph /mnt/source
[ceph@servera ~]$ dd if=/dev/zero of=/mnt/source/fileonsource bs=1M count=10

[ceph@servera ~]$ ls /mnt/source

[ceph@servera ~]$ sudo fsfreeze --freeze /mnt/source
[ceph@servera ~]$ rbd snap create rbd/clonetest@clonesnap
[ceph@servera ~]$ sudo fsfreeze --unfreeze /mnt/source
[ceph@servera ~]$ rbd snap protect rbd/clonetest@clonesnap
[ceph@servera ~]$ rbd snap ls rbd/clonetest

[ceph@servera ~]$ rbd clone rbd/clonetest@clonesnap rbd/realclone

[ceph@servera ~]$ sudo mkdir /mnt/clone
[ceph@servera ~]$ sudo rbd --id rbd.servera map rbd/realclone

[ceph@servera ~]$ sudo mount /dev/rbd1 /mnt/clone
[ceph@servera ~]$ df

[ceph@servera ~]$ dd if=/dev/zero of=/mnt/clone/fileonclone bs=1M count=10

[ceph@servera ~]$ ls -l /mnt/source

[ceph@servera ~]$ ls -l /mnt/clone

[ceph@servera ~]$ sudo umount /mnt/clone
[ceph@servera ~]$ sudo umount /mnt/source
[ceph@servera ~]$ df

[ceph@servera ~]$ sudo rbd --id rbd.servera unmap /dev/rbd1
[ceph@servera ~]$ sudo rbd --id rbd.servera unmap /dev/rbd0
[ceph@servera ~]$ rbd showmapped

[ceph@servera ~]$ rbd rm rbd/realclone

[ceph@servera ~]$ rbd snap unprotect rbd/clonetest@clonesnap
[ceph@servera ~]$ rbd snap purge rbd/clonetest

[ceph@servera ~]$ rbd rm rbd/clonetest

[ceph@servera ~]$ unset CEPH_ARGS



Pagina 150
GUIDED EXERCISE - CONFIGURING RBD MIRRORS
-----------------------------------------
[student@workstation ~]$ ssh student@servera

[ceph@servera ~]$ scp serverc:/etc/ceph/ceph.conf /etc/ceph/prod.conf
[ceph@servera ~]$ scp serverc:/etc/ceph/*admin*keyring \
/etc/ceph/prod.client.admin.keyring

[ceph@servera ~]$ scp serverf:/etc/ceph/ceph.conf /etc/ceph/bup.conf
[ceph@servera ~]$ scp serverf:/etc/ceph/*admin*keyring \
/etc/ceph/bup.client.admin.keyring

[ceph@servera ~]$ scp /etc/ceph/prod* serverf:/etc/ceph/
[ceph@servera ~]$ scp /etc/ceph/prod* serverc:/etc/ceph/

[ceph@servera ~]$ scp /etc/ceph/bup* serverf:/etc/ceph/
[ceph@servera ~]$ scp /etc/ceph/bup* serverc:/etc/ceph/

[ceph@servera ~]$ ceph -s --cluster prod

[ceph@servera ~]$ ceph -s --cluster bup

[ceph@servera ~]$ ceph osd pool create rbd 32 --cluster bup
[ceph@servera ~]$ rbd pool init rbd --cluster bup

[ceph@servera ~]$ ceph health --cluster bup

[student@servera ~]$ cd /home/student/serverf-cluster

[student@servera serverf-cluster]$ vi serverf-cluster-hosts
[mons]
serverf
[mgrs]
serverf
[osds]
serverf
[rbdmirrors]
serverf

[student@servera serverf-cluster]$ cp ./group_vars/rbd-mirrors.yml.sample \
./group_vars/rbd-mirrors.yml

[student@servera serverf-cluster]$ vi ./group_vars/rbd-mirrors.yml
---
dummy:
ceph_rbd_mirror_configure: true
ceph_rbd_mirror_pool: "rbd"
ceph_rbd_mirror_remote_cluster: "prod"
ceph_rbd_mirror_remote_user: "admin"

[student@servera serverf-cluster]$ ansible-playbook site.yml \
-i serverf-cluster-hosts

[student@servera serverf-cluster]$ ssh ceph@serverf
[ceph@serverf ~]$ ps -e | grep mirror

[ceph@serverf ~]$ rbd mirror pool enable rbd pool --cluster bup
[ceph@serverf ~]$ rbd mirror pool enable rbd pool --cluster prod

[ceph@serverf ~]$ rbd mirror pool peer add rbd client.admin@prod --cluster bup

[ceph@serverf ~]$ rbd mirror pool info rbd --cluster bup

[ceph@serverf ~]$ rbd mirror pool info rbd --cluster prod

[ceph@serverf ~]$ rbd mirror pool status rbd --cluster bup

[ceph@serverf ~]$ rbd mirror pool status rbd --cluster prod

[ceph@serverf ~]$ rbd create rbd/prod1 --size=128M \
--image-feature=exclusive-lock,journaling --cluster prod

[ceph@serverf ~]$ rbd mirror pool status rbd --cluster prod

[ceph@serverf ~]$ rbd mirror pool status rbd --cluster bup

[ceph@serverf ~]$ rbd ls --cluster prod

[ceph@serverf ~]$ rbd ls --cluster bup

[ceph@serverf ~]$ rbd mirror image status rbd/prod1 --cluster bup

[ceph@serverf ~]$ rbd mirror pool status rbd --cluster prod

[ceph@serverf ~]$ rbd rm rbd/prod1 --cluster prod

[ceph@serverf ~]$ rbd ls --cluster prod
[ceph@serverf ~]$ rbd ls --cluster bup

[ceph@serverf ~]$ ceph osd pool create vp 32 --cluster prod

[ceph@serverf ~]$ ceph osd pool create vp 32 --cluster bup

[ceph@serverf ~]$ rbd pool init vp --cluster prod
[ceph@serverf ~]$ rbd pool init vp --cluster bup

[ceph@serverf ~]$ rbd mirror pool enable vp image --cluster prod
[ceph@serverf ~]$ rbd mirror pool enable vp image --cluster bup
[ceph@serverf ~]$ rbd mirror pool peer add vp client.admin@prod --cluster bup

[ceph@serverf ~]$ rbd mirror pool info vp --cluster bup

[ceph@serverf ~]$ rbd mirror pool info vp --cluster prod

[ceph@serverf ~]$ rbd mirror pool status vp --cluster bup

[ceph@serverf ~]$ rbd mirror pool status vp --cluster prod

[ceph@serverf ~]$ rbd create vp/prod1 --size=128M \
--image-feature=exclusive-lock,journaling --cluster prod

[ceph@serverf ~]$ rbd -p vp ls --cluster prod

[ceph@serverf ~]$ rbd -p vp ls --cluster bup

[ceph@serverf ~]$ rbd mirror image enable vp/prod1 --cluster prod

[ceph@serverf ~]$ rbd -p vp ls --cluster bup

[ceph@serverf ~]$ rbd mirror image status vp/prod1 --cluster bup

[ceph@serverf ~]$ rbd rm vp/prod1 --cluster prod

[ceph@serverf ~]$ rbd -p vp ls --cluster prod
[ceph@serverf ~]$ rbd -p vp ls --cluster bup
[ceph@serverf ~]$ rm /etc/ceph/bup.* /etc/ceph/prod.*
[ceph@serverf ~]$ ssh serverc rm /etc/ceph/bup.* /etc/ceph/prod.*
[ceph@serverf ~]$ ssh servera rm /etc/ceph/bup.* /etc/ceph/prod.*

Pagina 173
GUIDED EXERCISE - IMPORTING AND EXPORTING RBD IMAGES
----------------------------------------------------
[ceph@serverc ~]$ ceph health

[ceph@serverc ~]$ ssh-keygen

[ceph@serverc ~]$ ssh-copy-id ceph@serverf

[ceph@serverc ~]$ ssh serverf

[ceph@serverf ~]$ ceph osd pool ls | grep rbd

[ceph@serverf ~]$ ceph osd pool create rbd 32

[ceph@serverf ~]$ rbd pool init rbd

[ceph@serverc ~]$ rbd create rbd/test --size=128M
[ceph@serverc ~]$ sudo rbd map rbd/test

[ceph@serverc ~]$ sudo mkfs.ext4 /dev/rbd0

[ceph@serverc ~]$ sudo mkdir /mnt/rbd
[ceph@serverc ~]$ sudo mount /dev/rbd0 /mnt/rbd
[ceph@serverc ~]$ sudo chown ceph:ceph /mnt/rbd
[ceph@serverc ~]$ cp /etc/ceph/ceph.conf /mnt/rbd/file0
[ceph@serverc ~]$ ls /mnt/rbd

[ceph@serverc ~]$ ls -l /mnt/rbd

[ceph@serverc ~]$ sudo umount /mnt/rbd

[ceph@serverc ~]$ rbd export rbd/test ~/export.dat

[ceph@serverc ~]$ ls -l ~/export.dat

[ceph@serverc ~]$ cat ~/export.dat | ssh ceph@serverf \
rbd import - rbd/test

[ceph@serverf ~]$ rbd du rbd/test

[ceph@serverf ~]$ sudo rbd map rbd/test

[ceph@serverf ~]$ sudo mkdir /mnt/rbd
[ceph@serverf ~]$ sudo mount /dev/rbd0 /mnt/rbd
[ceph@serverf ~]$ df

[ceph@serverf ~]$ ls -l /mnt/rbd

[ceph@serverf ~]$ cat /mnt/rbd/file0

[ceph@serverf ~]$ sudo umount /mnt/rbd
[ceph@serverf ~]$ sudo rbd unmap /dev/rbd0

[ceph@serverf ~]$ rbd snap create rbd/test@firstsnap
[ceph@serverf ~]$ exit

[ceph@serverc ~]$ rbd snap create rbd/test@firstsnap
[ceph@serverc ~]$ rbd du rbd/test

[ceph@serverc ~]$ sudo mount /dev/rbd0 /mnt/rbd
[ceph@serverc ~]$ cp /etc/services /mnt/rbd/file1
[ceph@serverc ~]$ cp /etc/resolv.conf /mnt/rbd/file2
[ceph@serverc ~]$ cp /etc/ceph/ceph.client.admin.keyring /mnt/rbd/file3
[ceph@serverc ~]$ ls -l /mnt/rbd

[ceph@serverc ~]$ rbd du rbd/test

[ceph@serverc ~]$ sudo umount /mnt/rbd

[ceph@serverc ~]$ rbd snap create rbd/test@secondsnap
[ceph@serverc ~]$ rbd du rbd/test

[ceph@serverc ~]$ rbd export-diff --from-snap \
firstsnap rbd/test@secondsnap - | ssh serverf rbd import-diff - rbd/test

[ceph@serverc ~]$ ssh serverf
[ceph@serverf ~]$ rbd du rbd/test

[ceph@serverf ~]$ sudo rbd map rbd/test

[ceph@serverf ~]$ sudo mount /dev/rbd0 /mnt/rbd
[ceph@serverf ~]$ df

[ceph@serverf ~]$ ls -l /mnt/rbd

[ceph@serverf ~]$ sudo umount /mnt/rbd
[ceph@serverf ~]$ sudo rbd unmap /dev/rbd0
[ceph@serverf ~]$ rbd snap purge rbd/test

[ceph@serverf ~]$ rbd rm rbd/test

[ceph@serverf ~]$ exit
[ceph@serverc ~]$ sudo rbd unmap /dev/rbd0
[ceph@serverc ~]$ rbd snap purge rbd/test
[ceph@serverc ~]$ rbd rm rbd/test


Pagina 180
LAB - PROVIDING BLOCK STORAGE WITH RBD
--------------------------------------
[student@workstation ~]$ lab ceph-rbd setup

[student@workstation ~]$ ssh ceph@serverc

[ceph@serverc ~]$ ceph osd pool create rbd125 32

[ceph@serverc ~]$ rbd pool init rbd125

[ceph@serverc ~]$ ceph osd pool ls detail | grep rbd125

[ceph@serverc ~]$ rbd create rbd125/prod125 --size=128M

[ceph@serverc ~]$ rbd -p rbd125 ls

[ceph@serverc ~]$ exit
[student@workstation ~]$ ssh ceph@servera

[ceph@servera ~]$ rbd -p rbd125 ls

[ceph@servera ~]$ sudo rbd map rbd125/prod125

[ceph@servera ~]$ sudo mkfs.ext4 /dev/rbd0

[ceph@servera ~]$ sudo mkdir /mnt/prod125

[ceph@servera ~]$ sudo mount /dev/rbd0 /mnt/prod125

[ceph@servera ~]$ sudo cp /etc/resolv.conf /mnt/prod125

[ceph@servera ~]$ ls /mnt/prod125

[ceph@servera ~]$ sudo umount /dev/rbd0
[ceph@servera ~]$ sudo rbd unmap rbd125/prod125

[ceph@servera ~]$ rbd snap create rbd125/prod125@beforeprod

[ceph@servera ~]$ rbd snap list rbd125/prod125

[ceph@servera ~]$ rbd export rbd125/prod125 /home/ceph/prod125.ext4

[ceph@servera ~]$ ls -lh /home/ceph/prod125.ext4

[ceph@servera ~]$ rbd import /home/ceph/prod125.ext4 rbd/img125

[ceph@servera ~]$ rbd ls

[student@workstation ~]$ lab ceph-rbd grade

[student@workstation ~]$ lab ceph-rbd cleanup